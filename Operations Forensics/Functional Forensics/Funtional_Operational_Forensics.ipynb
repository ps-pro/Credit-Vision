{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PLSGp4spfBr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqPZFkLyddSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy as cp\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "\n",
        "# Filter warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loadign Data\n",
        "df_accepted = cudf.read_csv(\"/content/drive/MyDrive/LendingClub Data/accepted_2007_to_2018Q4.csv\")"
      ],
      "metadata": {
        "id": "a1uXfLHEewcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Accepted data has {df_accepted.shape[0]} rows and {df_accepted.shape[1]} columns. \")"
      ],
      "metadata": {
        "id": "tod0lBzEf9Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def field_specific_null_analysis(df):\n",
        "    \"\"\"\n",
        "    Analyze null values for each column in the dataframe\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with columns: column_name, data_type, null_percentage, filled_percentage\n",
        "    \"\"\"\n",
        "    null_stats = []\n",
        "    total_rows = len(df)\n",
        "\n",
        "    for col in df.columns:\n",
        "        null_count = df[col].isnull().sum()\n",
        "        null_pct = (null_count / total_rows) * 100\n",
        "        filled_pct = 100 - null_pct\n",
        "\n",
        "        null_stats.append({\n",
        "            'column_name': col,\n",
        "            'data_type': str(df[col].dtype),\n",
        "            'null_percentage': round(null_pct, 2),\n",
        "            'filled_percentage': round(filled_pct, 2)\n",
        "        })\n",
        "\n",
        "    null_df = pd.DataFrame(null_stats)\n",
        "    null_df = null_df.sort_values(by='null_percentage', ascending=False).reset_index(drop=True)\n",
        "    display(null_df)"
      ],
      "metadata": {
        "id": "CBxdPvTsjlvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "field_specific_null_analysis(df_accepted)"
      ],
      "metadata": {
        "id": "D4T3tLJbk7Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_accepted['loan_status'].value_counts(normalize=True) * 100"
      ],
      "metadata": {
        "id": "8GP60-cDn0AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comprehensive_missing_analysis(df, target_col='loan_status'):\n",
        "    \"\"\"\n",
        "    Comprehensive missing value analysis for ALL columns\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Overall default and late rates for comparison\n",
        "    default_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off']\n",
        "    late_statuses = ['Late (16-30 days)', 'Late (31-120 days)']\n",
        "\n",
        "    # Calculate overall default and late rates\n",
        "    overall_default_rate = (df[target_col].isin(default_statuses).sum() / total_rows) * 100\n",
        "    overall_late_rate = (df[target_col].isin(late_statuses).sum() / total_rows) * 100\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col == target_col:\n",
        "            continue\n",
        "\n",
        "        missing_count = df[col].isnull().sum()\n",
        "        missing_pct = (missing_count / total_rows) * 100\n",
        "\n",
        "        # Calculate non-missing counts and percentages using notnull().sum()\n",
        "        non_missing_count = df[col].notnull().sum()\n",
        "        non_missing_pct = (non_missing_count / total_rows) * 100\n",
        "\n",
        "\n",
        "        if missing_count > 0:\n",
        "            # Analyze loan status for missing values\n",
        "            missing_mask = df[col].isnull()\n",
        "            missing_loan_status = df.loc[missing_mask, target_col].value_counts(normalize=True) * 100\n",
        "\n",
        "            # Calculate default and late rates for missing values\n",
        "            missing_default_rate = df.loc[missing_mask, target_col].isin(default_statuses).sum() / missing_count * 100\n",
        "            missing_late_rate = df.loc[missing_mask, target_col].isin(late_statuses).sum() / missing_count * 100\n",
        "\n",
        "            # Analyze loan status for non-missing values\n",
        "            non_missing_mask = df[col].notnull()\n",
        "            non_missing_default_rate = df.loc[non_missing_mask, target_col].isin(default_statuses).sum() / non_missing_count * 100\n",
        "            non_missing_late_rate = df.loc[non_missing_mask, target_col].isin(late_statuses).sum() / non_missing_count * 100\n",
        "\n",
        "\n",
        "            # Decision logic (can be refined based on late status analysis if needed)\n",
        "            if missing_pct >= 70:\n",
        "                recommendation = \"DROP - Too many missing values\"\n",
        "            elif missing_count < 100:  # Very few missing\n",
        "                 recommendation = \"DROP_ROWS - Very few missing observations\"\n",
        "            elif abs(missing_default_rate - non_missing_default_rate) > 2 or abs(missing_late_rate - non_missing_late_rate) > 2: # Significant difference in default or late rates\n",
        "                recommendation = \"TREAT_AS_CATEGORY - Missing is informative\"\n",
        "            elif abs(missing_default_rate - overall_default_rate) < 1 and abs(missing_late_rate - overall_late_rate) < 1: # Similar to overall default and late rates\n",
        "                recommendation = \"IMPUTE - Missing appears random\"\n",
        "            else:\n",
        "                recommendation = \"INVESTIGATE - Unclear pattern\"\n",
        "\n",
        "\n",
        "            results.append({\n",
        "                'column': col,\n",
        "                'missing_count': missing_count,\n",
        "                'missing_pct': round(missing_pct, 2),\n",
        "                'non_null_counts': non_missing_count,\n",
        "                'non_null_pct': round(non_missing_pct, 2),\n",
        "                'missing_default_rate': round(missing_default_rate, 2),\n",
        "                'non_missing_default_rate': round(non_missing_default_rate, 2),\n",
        "                'default_rate_diff': round(missing_default_rate - non_missing_default_rate, 2),\n",
        "                'missing_late_rate': round(missing_late_rate, 2),\n",
        "                'non_missing_late_rate': round(non_missing_late_rate, 2),\n",
        "                'late_rate_diff': round(missing_late_rate - non_missing_late_rate, 2),\n",
        "                'recommendation': recommendation\n",
        "            })\n",
        "        else: # Handle columns with no missing values\n",
        "             results.append({\n",
        "                'column': col,\n",
        "                'missing_count': 0,\n",
        "                'missing_pct': 0,\n",
        "                'non_null_counts': total_rows,\n",
        "                'non_null_pct': 100,\n",
        "                'missing_default_rate': None,\n",
        "                'non_missing_default_rate': None,\n",
        "                'default_rate_diff': None,\n",
        "                'missing_late_rate': None,\n",
        "                'non_missing_late_rate': None,\n",
        "                'late_rate_diff': None,\n",
        "                'recommendation': 'NO_MISSING - No action needed'\n",
        "            })\n",
        "\n",
        "\n",
        "\n",
        "    return pd.DataFrame(results).sort_values('missing_pct', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "JCcf4kg1MuhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detailed_column_missing_analysis(df, column, target_col='loan_status'):\n",
        "    \"\"\"\n",
        "    Deep dive analysis for a specific column's missing values\n",
        "    \"\"\"\n",
        "    print(f\"=== DETAILED ANALYSIS FOR: {column} ===\")\n",
        "\n",
        "    missing_count = df[column].isnull().sum()\n",
        "    total_rows = len(df)\n",
        "    missing_pct = (missing_count / total_rows) * 100\n",
        "\n",
        "    print(f\"Missing Values: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "    print(f\"Non-Missing Values: {total_rows - missing_count:,} ({100 - missing_pct:.2f}%)\")\n",
        "\n",
        "    if missing_count > 0:\n",
        "        print(\"\\n--- LOAN STATUS DISTRIBUTION FOR MISSING VALUES ---\")\n",
        "        missing_status_dist = df.loc[df[column].isnull(), target_col].value_counts(normalize=True) * 100\n",
        "        print(missing_status_dist.round(2))\n",
        "\n",
        "        print(\"\\n--- LOAN STATUS DISTRIBUTION FOR NON-MISSING VALUES ---\")\n",
        "        non_missing_status_dist = df.loc[df[column].notnull(), target_col].value_counts(normalize=True) * 100\n",
        "        print(non_missing_status_dist.round(2))\n",
        "\n",
        "        # Calculate default and late rates\n",
        "        default_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off']\n",
        "        late_statuses = ['Late (16-30 days)', 'Late (31-120 days)']\n",
        "\n",
        "\n",
        "        missing_default_rate = (df.loc[df[column].isnull(), target_col].isin(default_statuses).sum() / missing_count) * 100\n",
        "        non_missing_default_rate = (df.loc[df[column].notnull(), target_col].isin(default_statuses).sum() / (total_rows - missing_count)) * 100\n",
        "\n",
        "        missing_late_rate = (df.loc[df[column].isnull(), target_col].isin(late_statuses).sum() / missing_count) * 100\n",
        "        non_missing_late_rate = (df.loc[df[column].notnull(), target_col].isin(late_statuses).sum() / (total_rows - missing_count)) * 100\n",
        "\n",
        "\n",
        "        print(f\"\\n--- DEFAULT RATE COMPARISON ---\")\n",
        "        print(f\"Default Rate (Missing): {missing_default_rate:.2f}%\")\n",
        "        print(f\"Default Rate (Non-Missing): {non_missing_default_rate:.2f}%\")\n",
        "        print(f\"Difference: {missing_default_rate - non_missing_default_rate:.2f} percentage points\")\n",
        "\n",
        "\n",
        "        print(f\"\\n--- LATE RATE COMPARISON ---\")\n",
        "        print(f\"Late Rate (Missing): {missing_late_rate:.2f}%\")\n",
        "        print(f\"Late Rate (Non-Missing): {non_missing_late_rate:.2f}%\")\n",
        "        print(f\"Difference: {missing_late_rate - non_missing_late_rate:.2f} percentage points\")\n",
        "\n",
        "\n",
        "        if abs(missing_default_rate - non_missing_default_rate) > 2 or abs(missing_late_rate - non_missing_late_rate) > 2:\n",
        "            print(\"SIGNIFICANT DIFFERENCE - Missing values are informative!\")\n",
        "        else:\n",
        "            print(\"Similar default and late rates - Missing appears random\")\n",
        "\n",
        "\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "RHusmGR9oBUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive analysis of ALL columns\n",
        "missing_analysis_df = comprehensive_missing_analysis(df_accepted)\n",
        "display(missing_analysis_df)"
      ],
      "metadata": {
        "id": "PWFKRDUHrttU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_column_missing_analysis(df_accepted,\"bc_util\")"
      ],
      "metadata": {
        "id": "2mIi8xSjtx9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_analysis_df = comprehensive_missing_analysis(df_accepted)\n",
        "correlation = missing_analysis_df['default_rate_diff'].corr(missing_analysis_df['late_rate_diff'])\n",
        "print(f\"Correlation between Default Rate Difference and Late Rate Difference: {correlation:.2f}\")"
      ],
      "metadata": {
        "id": "p9w-EsS2t3na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_data_sections(df_accepted: cudf.DataFrame,\n",
        "                          missing_analysis_df: pd.DataFrame,\n",
        "                          id_column: str = 'id',\n",
        "                          tolerance: int = 1000,\n",
        "                          min_cols_in_section: int = 2):\n",
        "    \"\"\"\n",
        "    Analyzes a DataFrame to identify sections of data based on patterns of missing values.\n",
        "\n",
        "    This function groups columns that have a similar number of non-null values, identifies\n",
        "    the samples (rows) corresponding to these groups, and reports on the size of these\n",
        "    unique sample sections.\n",
        "\n",
        "    The process works from columns with the most missing data to the least. Samples are\n",
        "    assigned to the first (most specific) section they qualify for, preventing double-counting\n",
        "    and ensuring that the resulting sections of samples are disjoint.\n",
        "\n",
        "    Args:\n",
        "        df_accepted (cudf.DataFrame): The main DataFrame containing the data and a unique identifier.\n",
        "                                      Must be a cuDF DataFrame.\n",
        "        missing_analysis_df (pd.DataFrame): A pandas DataFrame containing the missing value analysis,\n",
        "                                            including 'column', 'non_null_counts', and 'missing_pct'.\n",
        "                                            This DataFrame must be sorted by 'non_null_counts' ascending.\n",
        "        id_column (str): The name of the unique identifier column in df_accepted. Defaults to 'id'.\n",
        "        tolerance (int): The maximum difference in non_null_counts for columns to be\n",
        "                         considered part of the same group. Defaults to 1000.\n",
        "        min_cols_in_section (int): The minimum number of columns required to form a section.\n",
        "                                   Defaults to 2.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the required columns exist in the analysis dataframe\n",
        "    required_cols = ['column', 'non_null_counts', 'missing_pct']\n",
        "    if not all(col in missing_analysis_df.columns for col in required_cols):\n",
        "        raise ValueError(f\"missing_analysis_df must contain the columns: {required_cols}\")\n",
        "\n",
        "    # Ensure the id_column exists in the main dataframe\n",
        "    if id_column not in df_accepted.columns:\n",
        "        raise ValueError(f\"The specified id_column '{id_column}' does not exist in df_accepted.\")\n",
        "\n",
        "    print(f\"Starting section analysis with tolerance={tolerance} and min_cols_in_section={min_cols_in_section}.\\n\")\n",
        "\n",
        "    # --- Step 1: Group columns based on similar non_null_counts ---\n",
        "\n",
        "    analysis_rows = missing_analysis_df.to_dict('records')\n",
        "    processed_indices = set()\n",
        "    column_groups = []\n",
        "\n",
        "    for i in range(len(analysis_rows)):\n",
        "        if i in processed_indices:\n",
        "            continue\n",
        "\n",
        "        seed_row = analysis_rows[i]\n",
        "        if seed_row['non_null_counts'] == 0:\n",
        "            processed_indices.add(i)\n",
        "            continue\n",
        "\n",
        "        seed_count = seed_row['non_null_counts']\n",
        "        current_group = [seed_row]\n",
        "        processed_indices.add(i)\n",
        "\n",
        "        for j in range(i + 1, len(analysis_rows)):\n",
        "            if j in processed_indices:\n",
        "                continue\n",
        "\n",
        "            candidate_row = analysis_rows[j]\n",
        "            if abs(candidate_row['non_null_counts'] - seed_count) <= tolerance:\n",
        "                current_group.append(candidate_row)\n",
        "                processed_indices.add(j)\n",
        "\n",
        "        if len(current_group) >= min_cols_in_section:\n",
        "            column_groups.append(current_group)\n",
        "\n",
        "    print(f\"Found {len(column_groups)} column groups meeting the criteria.\")\n",
        "\n",
        "    # --- Step 2: Identify disjoint sets of samples for each column group ---\n",
        "\n",
        "    unassigned_ids = df_accepted[[id_column]].copy()\n",
        "    section_results = []\n",
        "\n",
        "    for i, group in enumerate(column_groups):\n",
        "        column_names = [row['column'] for row in group]\n",
        "\n",
        "        mask = df_accepted[column_names[0]].notnull()\n",
        "        for col in column_names[1:]:\n",
        "            mask &= df_accepted[col].notnull()\n",
        "\n",
        "        potential_section_ids = df_accepted.loc[mask, [id_column]]\n",
        "        new_section_ids = unassigned_ids.merge(potential_section_ids, on=id_column, how='inner')\n",
        "        num_new_samples = len(new_section_ids)\n",
        "\n",
        "        if num_new_samples > 0:\n",
        "            section_info = {\n",
        "                'section_index': len(section_results) + 1,\n",
        "                'num_samples': num_new_samples,\n",
        "                'defining_columns': [f\"{row['column']} [{row['missing_pct']}%]\" for row in group]\n",
        "            }\n",
        "            section_results.append(section_info)\n",
        "\n",
        "            if not new_section_ids.empty:\n",
        "                ids_to_remove = new_section_ids[id_column]\n",
        "                isin_mask = unassigned_ids[id_column].isin(ids_to_remove)\n",
        "                unassigned_ids = unassigned_ids[~isin_mask]\n",
        "\n",
        "    # --- Step 3: Report the results ---\n",
        "\n",
        "    print(\"\\n--- Section Analysis Results ---\\n\")\n",
        "    if not section_results:\n",
        "        print(\"No sections were identified based on the provided criteria.\")\n",
        "\n",
        "    for result in section_results:\n",
        "        print(f\"Section {result['section_index']}:\")\n",
        "        print(f\"  - Number of unique samples: {result['num_samples']:,}\")\n",
        "        print(\"  - Defining Columns (and their total missing %):\")\n",
        "        for col_info in result['defining_columns']:\n",
        "            print(f\"    - {col_info}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    remaining_samples = len(unassigned_ids)\n",
        "    if remaining_samples > 0:\n",
        "        print(\"Remaining Samples:\")\n",
        "        print(f\"  - Number of samples: {remaining_samples:,}\")\n",
        "        print(\"  - These samples did not exclusively fit into the high-missingness patterns above.\")\n",
        "        print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "Gkmrxm7tYXkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The missing_analysis_df must be sorted by non_null_counts ascending\n",
        "missing_analysis_df_sorted = missing_analysis_df.sort_values('non_null_counts', ascending=True).reset_index(drop=True)\n",
        "\n",
        "# Run the analysis\n",
        "analyze_data_sections(df_accepted, missing_analysis_df_sorted, id_column='id')"
      ],
      "metadata": {
        "id": "6AW6-m-aXz5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cudf\n",
        "import cupy as cp\n",
        "\n",
        "def create_data_sections(df_accepted: cudf.DataFrame,\n",
        "                         missing_analysis_df: pd.DataFrame,\n",
        "                         id_column: str = 'id',\n",
        "                         tolerance: int = 1000,\n",
        "                         min_cols_in_section: int = 2):\n",
        "    \"\"\"\n",
        "    Identifies and creates disjoint data sections based on missing value patterns.\n",
        "\n",
        "    This function groups columns with similar non-null counts, identifies the unique\n",
        "    sample sets for each group, and returns these sections as separate DataFrames.\n",
        "    The process is prioritized, starting with sections defined by columns with the\n",
        "    most missing data.\n",
        "\n",
        "    Args:\n",
        "        df_accepted (cudf.DataFrame): The main DataFrame containing the data. A unique\n",
        "                                      identifier column is required.\n",
        "        missing_analysis_df (pd.DataFrame): A DataFrame from the initial analysis, containing\n",
        "                                            'column', 'non_null_counts', and 'missing_pct'.\n",
        "                                            Must be sorted by 'non_null_counts' ascending.\n",
        "        id_column (str): The name of the unique identifier column in df_accepted.\n",
        "        tolerance (int): The max difference in non_null_counts for columns to form a group.\n",
        "        min_cols_in_section (int): The minimum number of columns to define a section.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "        - list: A list of dictionary objects. Each dictionary represents a section and contains:\n",
        "            - 'priority' (int): The priority of the section (1 is highest).\n",
        "            - 'section_info' (dict): Metadata about the section (sample count, defining columns).\n",
        "            - 'section_df' (cudf.DataFrame): The actual data for that section.\n",
        "        - cudf.DataFrame: A DataFrame containing all samples that did not fit into any\n",
        "                          of the defined sections.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Initial Validation and Setup ---\n",
        "    required_cols = ['column', 'non_null_counts', 'missing_pct']\n",
        "    if not all(col in missing_analysis_df.columns for col in required_cols):\n",
        "        raise ValueError(f\"missing_analysis_df must contain the columns: {required_cols}\")\n",
        "\n",
        "    if id_column not in df_accepted.columns:\n",
        "        raise ValueError(f\"The specified id_column '{id_column}' does not exist in df_accepted.\")\n",
        "\n",
        "    print(f\"Starting section creation with tolerance={tolerance} and min_cols_in_section={min_cols_in_section}.\\n\")\n",
        "\n",
        "    # --- Step 1: Group columns based on similar non_null_counts ---\n",
        "    analysis_rows = missing_analysis_df.to_dict('records')\n",
        "    processed_indices = set()\n",
        "    column_groups = []\n",
        "\n",
        "    for i in range(len(analysis_rows)):\n",
        "        if i in processed_indices: continue\n",
        "        seed_row = analysis_rows[i]\n",
        "        if seed_row['non_null_counts'] == 0:\n",
        "            processed_indices.add(i)\n",
        "            continue\n",
        "\n",
        "        seed_count = seed_row['non_null_counts']\n",
        "        current_group = [seed_row]\n",
        "        processed_indices.add(i)\n",
        "\n",
        "        for j in range(i + 1, len(analysis_rows)):\n",
        "            if j in processed_indices: continue\n",
        "            candidate_row = analysis_rows[j]\n",
        "            if abs(candidate_row['non_null_counts'] - seed_count) <= tolerance:\n",
        "                current_group.append(candidate_row)\n",
        "                processed_indices.add(j)\n",
        "\n",
        "        if len(current_group) >= min_cols_in_section:\n",
        "            column_groups.append(current_group)\n",
        "\n",
        "    print(f\"Found {len(column_groups)} potential column groups.\")\n",
        "\n",
        "    # --- Step 2: Identify disjoint sample IDs and create DataFrames ---\n",
        "\n",
        "    unassigned_ids = df_accepted[[id_column]].copy()\n",
        "    sections_output = []\n",
        "\n",
        "    for group in column_groups:\n",
        "        column_names = [row['column'] for row in group]\n",
        "\n",
        "        mask = df_accepted[column_names[0]].notnull()\n",
        "        for col in column_names[1:]:\n",
        "            mask &= df_accepted[col].notnull()\n",
        "\n",
        "        potential_section_ids = df_accepted.loc[mask, [id_column]]\n",
        "        new_section_ids = unassigned_ids.merge(potential_section_ids, on=id_column, how='inner')\n",
        "        num_new_samples = len(new_section_ids)\n",
        "\n",
        "        if num_new_samples > 0:\n",
        "            # Create the DataFrame for this section by filtering the original df\n",
        "            # This merge acts as a filter to get all columns for the identified IDs\n",
        "            section_df = new_section_ids.merge(df_accepted, on=id_column, how='left')\n",
        "\n",
        "            # Prepare the metadata object for this section\n",
        "            section_priority = len(sections_output) + 1\n",
        "            section_info = {\n",
        "                'num_samples': num_new_samples,\n",
        "                'defining_columns': [f\"{row['column']} [{row['missing_pct']}%]\" for row in group]\n",
        "            }\n",
        "\n",
        "            sections_output.append({\n",
        "                'priority': section_priority,\n",
        "                'section_info': section_info,\n",
        "                'section_df': section_df.copy() # Explicitly copy to ensure it's a new object\n",
        "            })\n",
        "\n",
        "            # Remove these newly assigned IDs from the pool of unassigned IDs\n",
        "            ids_to_remove = new_section_ids[id_column]\n",
        "            isin_mask = unassigned_ids[id_column].isin(ids_to_remove)\n",
        "            unassigned_ids = unassigned_ids[~isin_mask]\n",
        "\n",
        "    # --- Step 3: Create DataFrame for remaining samples ---\n",
        "\n",
        "    # The remaining unassigned_ids now define the \"rest\" of the data\n",
        "    remaining_df = unassigned_ids.merge(df_accepted, on=id_column, how='left').copy()\n",
        "\n",
        "    print(f\"\\nSuccessfully created {len(sections_output)} data sections.\")\n",
        "    print(f\"Found {len(remaining_df)} remaining samples.\")\n",
        "\n",
        "    return sections_output, remaining_df"
      ],
      "metadata": {
        "id": "vsY8XrBPaLuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to get the sections and the remainder\n",
        "data_sections, df_remaining = create_data_sections(\n",
        "    df_accepted,\n",
        "    missing_analysis_df_sorted,\n",
        "    id_column='id'\n",
        ")"
      ],
      "metadata": {
        "id": "cYVo9ny8aTSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Now you can work with the results ---\n",
        "\n",
        "print(\"\\n--- Accessing the Created Sections ---\")\n",
        "\n",
        "# Check the number of sections created\n",
        "print(f\"Total sections created: {len(data_sections)}\")\n",
        "\n",
        "# Access the section\n",
        "\n",
        "SECTION_NUMBER = 7\n",
        "\n",
        "if data_sections:\n",
        "    section = data_sections[SECTION_NUMBER]\n",
        "    print(f\"\\nPriority of section number {SECTION_NUMBER} : {section['priority']}\")\n",
        "    print(f\"Number of samples in section number {SECTION_NUMBER} : {section['section_info']['num_samples']}\")\n",
        "    print(\"Defining columns for first section:\")\n",
        "    for col_info in section['section_info']['defining_columns']:\n",
        "        print(f\"  - {col_info}\")\n",
        "\n",
        "    print(f\"\\n Section number {SECTION_NUMBER} DataFrame info:\")\n",
        "    section['section_df'].info()"
      ],
      "metadata": {
        "id": "fCqhVmaJYiOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the remaining samples DataFrame\n",
        "print(\"\\n--- Remaining Samples DataFrame ---\")\n",
        "print(f\"Shape of the remaining samples DataFrame: {df_remaining.shape}\")\n",
        "df_remaining.info()"
      ],
      "metadata": {
        "id": "sIYwzsmDafP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_section_overlap_matrix(data_sections: list,\n",
        "                               df_remaining: cudf.DataFrame,\n",
        "                               id_column: str = 'id') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the number of common samples (ID overlap) between all data sections.\n",
        "\n",
        "    This function generates a confusion matrix where each cell (i, j) contains the\n",
        "    number of unique IDs that are present in both section i and section j. The diagonal\n",
        "    (i, i) will contain the total number of samples in that section.\n",
        "\n",
        "    Args:\n",
        "        data_sections (list): The list of section objects produced by create_data_sections.\n",
        "        df_remaining (cudf.DataFrame): The DataFrame of samples not in any other section.\n",
        "        id_column (str): The name of the unique identifier column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame representing the overlap matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Prepare a list of all DataFrames and their labels ---\n",
        "    all_dfs = [sec['section_df'] for sec in data_sections]\n",
        "    labels = [f\"Section {sec['priority']}\" for sec in data_sections]\n",
        "\n",
        "    if not df_remaining.empty:\n",
        "        all_dfs.append(df_remaining)\n",
        "        labels.append(\"Remaining\")\n",
        "\n",
        "    num_sections = len(all_dfs)\n",
        "    print(f\"Analyzing overlap across {num_sections} total dataframes...\")\n",
        "\n",
        "    # --- Pre-fetch all IDs and store them in sets for efficient intersection ---\n",
        "    id_sets = [set(df[id_column].to_pandas()) for df in all_dfs]\n",
        "\n",
        "    # --- Initialize the matrix ---\n",
        "    overlap_matrix = pd.DataFrame(index=labels, columns=labels, dtype=int)\n",
        "\n",
        "    # --- Populate the matrix ---\n",
        "    for i in range(num_sections):\n",
        "        for j in range(num_sections):\n",
        "            if i == j:\n",
        "                # The diagonal contains the total size of the section\n",
        "                overlap_matrix.iloc[i, j] = len(id_sets[i])\n",
        "            else:\n",
        "                # Off-diagonals contain the size of the intersection\n",
        "                intersection = id_sets[i].intersection(id_sets[j])\n",
        "                overlap_matrix.iloc[i, j] = len(intersection)\n",
        "\n",
        "    print(\"Overlap analysis complete.\")\n",
        "    return overlap_matrix"
      ],
      "metadata": {
        "id": "-5UQcJiDbtgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Calculate the overlap data matrix ---\n",
        "overlap_df = get_section_overlap_matrix(data_sections, df_remaining, id_column='id')"
      ],
      "metadata": {
        "id": "eUVZM80VcCje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRaw Overlap Data:\")\n",
        "display(overlap_df)"
      ],
      "metadata": {
        "id": "71_WIdhscIrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def verify_total_sample_count(overlap_matrix: pd.DataFrame,\n",
        "                                original_df: cudf.DataFrame):\n",
        "    \"\"\"\n",
        "    Verifies that the sum of all disjoint sections equals the total number of samples\n",
        "    in the original DataFrame.\n",
        "\n",
        "    Args:\n",
        "        overlap_matrix (pd.DataFrame): The matrix from get_section_overlap_matrix.\n",
        "                                       Its diagonal contains the size of each section.\n",
        "        original_df (cudf.DataFrame): The original, complete DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Calculation ---\n",
        "    sum_of_sections = np.diag(overlap_matrix.values).sum()\n",
        "    total_original_samples = len(original_df)\n",
        "\n",
        "    # --- Verification and Reporting ---\n",
        "    print(\"--- Final Data Integrity Check ---\")\n",
        "    print(f\"Sum of all samples in all created sections: {int(sum_of_sections):,}\")\n",
        "    print(f\"Total samples in the original DataFrame:    {total_original_samples:,}\")\n",
        "\n",
        "    if int(sum_of_sections) == total_original_samples:\n",
        "        print(\"\\nSUCCESS: The total number of samples matches perfectly.\")\n",
        "        print(\"This confirms that every sample from the original DataFrame has been accounted for.\")\n",
        "    else:\n",
        "        difference = total_original_samples - int(sum_of_sections)\n",
        "        print(f\"\\nERROR: There is a mismatch of {difference:,} samples.\")\n",
        "        print(\"This indicates that some data was lost or duplicated during the process.\")\n",
        "    print(\"------------------------------------\")\n",
        "\n",
        "\n",
        "# --- How to run the verification ---\n",
        "# (Assuming overlap_df and df_accepted are already created)\n",
        "\n",
        "verify_total_sample_count(overlap_df, df_accepted)"
      ],
      "metadata": {
        "id": "aNGFI6HScOPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "def analyze_section_nulls(data_sections: list, df_remaining: cudf.DataFrame, section_number: int = None):\n",
        "    \"\"\"\n",
        "    Performs and displays a detailed null value analysis for specific data sections.\n",
        "\n",
        "    For a given section DataFrame (or all sections if section_number is None)\n",
        "    and the remaining DataFrame, this function calculates the number and percentage\n",
        "    of null values for every column relative to the size of that specific section.\n",
        "\n",
        "    Args:\n",
        "        data_sections (list): The list of section objects from create_data_sections.\n",
        "        df_remaining (cudf.DataFrame): The DataFrame of samples not in any other section.\n",
        "        section_number (int, optional): The priority number of the section to analyze.\n",
        "                                        If None, all sections and the remaining data\n",
        "                                        will be analyzed. Defaults to None.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Combine all dataframes into a single list for easier iteration ---\n",
        "    all_section_data = []\n",
        "    for section in data_sections:\n",
        "        # Create a descriptor for each section using its priority\n",
        "        label = f\"Section {section['priority']}\"\n",
        "        df = section['section_df']\n",
        "        all_section_data.append({'label': label, 'df': df, 'priority': section['priority']})\n",
        "\n",
        "    # Add the 'Remaining' dataframe to the list if it's not empty\n",
        "    if not df_remaining.empty:\n",
        "        all_section_data.append({'label': 'Remaining', 'df': df_remaining, 'priority': None}) # Remaining has no priority number\n",
        "\n",
        "    # --- Iterate through each dataframe and perform the analysis ---\n",
        "    for section_info in all_section_data:\n",
        "        # Skip sections if a specific section number is requested and it doesn't match\n",
        "        if section_number is not None and section_info['priority'] != section_number:\n",
        "            continue\n",
        "\n",
        "        df = section_info['df']\n",
        "        label = section_info['label']\n",
        "        total_rows = len(df)\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Null Value Analysis for: {label} ({total_rows:,} Samples)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if total_rows == 0:\n",
        "            print(\"Section is empty. No analysis to perform.\")\n",
        "            continue\n",
        "\n",
        "        # --- Calculate nulls for each column in the current dataframe ---\n",
        "        null_stats = []\n",
        "        for col in df.columns:\n",
        "            # .isnull().sum() on a cuDF series returns a scalar value\n",
        "            # .item() safely converts it to a standard Python number\n",
        "            null_count = df[col].isnull().sum().item()\n",
        "            null_pct = (null_count / total_rows) * 100\n",
        "\n",
        "            null_stats.append({\n",
        "                'Column': col,\n",
        "                'Null Count': null_count,\n",
        "                'Null Percentage (%)': round(null_pct, 2)\n",
        "            })\n",
        "\n",
        "        # Create a pandas DataFrame for better display and sorting\n",
        "        analysis_table = pd.DataFrame(null_stats)\n",
        "\n",
        "        # Sort the table to show columns with the most missing values on top\n",
        "        analysis_table = analysis_table.sort_values(\n",
        "            by='Null Percentage (%)',\n",
        "            ascending=False\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        # Use display() for clean, interactive table formatting in a notebook\n",
        "        display(analysis_table)"
      ],
      "metadata": {
        "id": "88UQpzMudUOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze_section_nulls(data_sections, df_remaining) # Analyze all sections\n",
        "analyze_section_nulls(data_sections, df_remaining, section_number=11) # Analyze only Section 1"
      ],
      "metadata": {
        "id": "QHAe_rmmd8ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Exhibit A-1: Risk-Adjusted Pricing Forensics\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Import necessary libraries for analysis ---\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STEP 0: DEFINE THE COHORTS FOR ANALYSIS\n",
        "# ------------------------------------------------------------------------------\n",
        "# Based on our approved strategy, we are focusing on the first 7 sections\n",
        "# as they are statistically significant and represent distinct business processes.\n",
        "\n",
        "cohort_definitions = {\n",
        "    \"1: Hardship\": data_sections[0]['section_df'],\n",
        "    \"2: Settlement\": data_sections[1]['section_df'],\n",
        "    \"3: Joint App (Full Profile)\": data_sections[2]['section_df'],\n",
        "    \"4: Joint App (Income Only)\": data_sections[3]['section_df'],\n",
        "    \"5: Individual (Enriched Data)\": data_sections[4]['section_df'],\n",
        "    \"6: Individual (Bankcard Data)\": data_sections[5]['section_df'],\n",
        "    \"7: Individual (Legacy Data)\": data_sections[6]['section_df']\n",
        "}\n",
        "\n",
        "print(f\"Successfully defined {len(cohort_definitions)} cohorts for forensic analysis.\")"
      ],
      "metadata": {
        "id": "hCJTkauOSJJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# STEP 1: VISUAL DIAGNOSTICS - Linearity Check\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def create_diagnostic_scatter_plots(cohorts: dict):\n",
        "    \"\"\"Creates a grid of scatter plots to visually inspect the linearity assumption.\"\"\"\n",
        "    print(\"\\nExecuting Step 1: Generating visual diagnostics for linearity assumption...\")\n",
        "\n",
        "    # Create a 3x3 grid for our 7 cohorts\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=3,\n",
        "        subplot_titles=list(cohorts.keys()),\n",
        "        vertical_spacing=0.1,\n",
        "        horizontal_spacing=0.08\n",
        "    )\n",
        "\n",
        "    row, col = 1, 1\n",
        "    for name, df_cudf in cohorts.items():\n",
        "        sample_size = len(df_cudf)\n",
        "        df_pd = df_cudf[['fico_range_high', 'int_rate']].sample(n=sample_size).to_pandas().dropna()\n",
        "\n",
        "        # Create scatter plot\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df_pd['fico_range_high'], y=df_pd['int_rate'],\n",
        "            mode='markers', marker=dict(size=3, opacity=0.4), name=name\n",
        "        ), row=row, col=col)\n",
        "\n",
        "        # Add regression line\n",
        "        trendline = px.scatter(df_pd, x='fico_range_high', y='int_rate', trendline=\"ols\").data[1]\n",
        "        fig.add_trace(trendline, row=row, col=col)\n",
        "\n",
        "        if col == 3:\n",
        "            col = 1\n",
        "            row += 1\n",
        "        else:\n",
        "            col += 1\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=\"<b>Exhibit A-1.1: Visual Diagnostic of FICO vs. Interest Rate Linearity</b><br><sup>(Per-Cohort Analysis)</sup>\",\n",
        "        height=800, width=1200, showlegend=False\n",
        "    )\n",
        "    fig.update_xaxes(title_text=\"FICO Score (High Range)\")\n",
        "    fig.update_yaxes(title_text=\"Interest Rate (%)\")\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "D_JF7FTtSFd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execute Step 1 ---\n",
        "create_diagnostic_scatter_plots(cohort_definitions)"
      ],
      "metadata": {
        "id": "tOseak5pSREY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# STEP 2 & 3: CORE ANALYSIS - R-squared and RMSE Calculation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_pricing_analysis_per_cohort(cohorts: dict):\n",
        "    \"\"\"Runs regression for each cohort to calculate R-squared and RMSE.\"\"\"\n",
        "    print(\"\\nExecuting Steps 2 & 3: Calculating R-squared and RMSE for each cohort...\")\n",
        "\n",
        "    results = []\n",
        "    for name, df_cudf in cohorts.items():\n",
        "        # Convert only necessary columns to pandas to save memory, and drop nulls\n",
        "        df_pd = df_cudf[['fico_range_high', 'int_rate']].dropna().to_pandas()\n",
        "\n",
        "        if len(df_pd) < 2:\n",
        "            print(f\"  -> Skipping cohort '{name}' due to insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        X = df_pd[['fico_range_high']]\n",
        "        y = df_pd['int_rate']\n",
        "\n",
        "        # Fit the linear regression model\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Calculate metrics\n",
        "        r_squared = model.score(X, y)\n",
        "        predictions = model.predict(X)\n",
        "        rmse = np.sqrt(mean_squared_error(y, predictions))\n",
        "\n",
        "        results.append({\n",
        "            \"Cohort\": name,\n",
        "            \"Sample Size (n)\": len(df_pd),\n",
        "            \"R-squared\": r_squared,\n",
        "            \"RMSE (int_rate %)\": rmse\n",
        "        })\n",
        "        print(f\"  -> Analysis complete for cohort: {name}\")\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "AD6N0X8ZSTLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execute Steps 2 & 3 ---\n",
        "analysis_results_df = run_pricing_analysis_per_cohort(cohort_definitions)\n",
        "\n",
        "print(\"\\n--- Forensic Summary Table ---\")\n",
        "display(analysis_results_df.sort_values(by=\"R-squared\", ascending=False).reset_index(drop=True))"
      ],
      "metadata": {
        "id": "DDZGlihYSWK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# STEP 4: FINAL VISUALIZATION - R-squared Comparison\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def create_r_squared_bar_chart(results_df: pd.DataFrame):\n",
        "    \"\"\"Creates the final summary bar chart of R-squared values.\"\"\"\n",
        "    print(\"\\nExecuting Step 4: Generating final R-squared comparison chart...\")\n",
        "\n",
        "    results_df = results_df.sort_values(by=\"R-squared\", ascending=False)\n",
        "\n",
        "    fig = px.bar(\n",
        "        results_df,\n",
        "        x='Cohort',\n",
        "        y='R-squared',\n",
        "        text=results_df['R-squared'].apply(lambda x: f'{x:.3f}'),\n",
        "        color='R-squared',\n",
        "        color_continuous_scale='Blues',\n",
        "        labels={'R-squared': 'R-squared (FICO vs. Interest Rate)'}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=\"<b>Exhibit A-1.2: FICO's Explanatory Power on Interest Rate Pricing</b><br><sup>(A Higher R-squared Implies a More FICO-driven Pricing Model)</sup>\",\n",
        "        yaxis_range=[0,1]\n",
        "    )\n",
        "    fig.update_traces(textposition='outside')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "abBcEI2gSFbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execute Step 4 ---\n",
        "create_r_squared_bar_chart(analysis_results_df)"
      ],
      "metadata": {
        "id": "1a8Wbd_tScgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1A DONE\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YltKuxFVY7Fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# 2A\n"
      ],
      "metadata": {
        "id": "5C4SvJTVZBD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EXHIBIT A-2: VINTAGE ANALYSIS FORENSICS\n",
        "# Lending Club Operational Forensics Project\n",
        "# ==============================================================================\n",
        "# Objective: Investigate the impact of the 2012 \"Great Data Enrichment\" by\n",
        "# comparing the underwriting inputs and performance outcomes of two distinct\n",
        "# loan vintages from the \"Core Individual Cohort\".\n",
        "#\n",
        "# Key Assumption: Date of Default = Last Payment Date + 1 month\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Import necessary libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, chi2_contingency\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXHIBIT A-2: VINTAGE ANALYSIS FORENSICS\")\n",
        "print(\"Assessing the Impact of the 2012 'Great Data Enrichment'\")\n",
        "print(\"=\" * 80)\n",
        "print()"
      ],
      "metadata": {
        "id": "wh5iXH9cfs3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: CONSTRUCT THE \"CORE INDIVIDUAL COHORT\"\n",
        "# ==============================================================================\n",
        "print(\"STEP 1: Constructing the 'Core Individual Cohort'\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Concatenate all individual loan sections (excluding joint/hardship/settlement)\n",
        "core_individual_cohort_pd = pd.concat([\n",
        "    data_sections[4]['section_df'].to_pandas(),  # Individual (Enriched Data)\n",
        "    data_sections[5]['section_df'].to_pandas(),  # Individual (Bankcard Data)\n",
        "    data_sections[6]['section_df'].to_pandas(),  # Individual (Legacy Data)\n",
        "    data_sections[7]['section_df'].to_pandas(),  # Additional individual sections\n",
        "    data_sections[8]['section_df'].to_pandas(),\n",
        "    data_sections[9]['section_df'].to_pandas(),\n",
        "    data_sections[10]['section_df'].to_pandas(),\n",
        "    df_remaining.to_pandas()  # Remaining unclassified individual loans\n",
        "], ignore_index=True)"
      ],
      "metadata": {
        "id": "XNS0OCHofuhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date columns to datetime immediately after concatenation\n",
        "print(\"Converting date columns...\")\n",
        "core_individual_cohort_pd['issue_d'] = pd.to_datetime(\n",
        "    core_individual_cohort_pd['issue_d'],\n",
        "    format='%b-%Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "core_individual_cohort_pd['last_pymnt_d'] = pd.to_datetime(\n",
        "    core_individual_cohort_pd['last_pymnt_d'],\n",
        "    format='%b-%Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "print(f\" Core Individual Cohort constructed\")\n",
        "print(f\"   Total samples: {len(core_individual_cohort_pd):,}\")\n",
        "# Filter out rows where 'issue_d' could not be converted to datetime (NaT) before finding min/max\n",
        "valid_issue_dates = core_individual_cohort_pd['issue_d'].dropna()\n",
        "if not valid_issue_dates.empty:\n",
        "    print(f\"   Date range: {valid_issue_dates.min().strftime('%b-%Y')} to {valid_issue_dates.max().strftime('%b-%Y')}\")\n",
        "else:\n",
        "    print(\"   Date range: No valid issue dates found after conversion.\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "vYtBh7xBg9-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 2: DATA PREPARATION & VINTAGE DEFINITION\n",
        "# ==============================================================================\n",
        "print(\"STEP 2: Data Preparation & Vintage Definition\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Convert date columns to datetime\n",
        "print(\"Converting date columns...\")\n",
        "core_individual_cohort_pd['issue_d'] = pd.to_datetime(\n",
        "    core_individual_cohort_pd['issue_d'],\n",
        "    format='%b-%Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "core_individual_cohort_pd['last_pymnt_d'] = pd.to_datetime(\n",
        "    core_individual_cohort_pd['last_pymnt_d'],\n",
        "    format='%b-%Y',\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# Data snapshot date (end of dataset)\n",
        "snapshot_date = pd.to_datetime('2018-12-31')\n",
        "\n",
        "# Define default statuses\n",
        "default_statuses = ['Charged Off', 'Default']"
      ],
      "metadata": {
        "id": "QAz77lz5fxOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CALCULATE DEFAULT TIMING WITH OUR ASSUMPTION\n",
        "# ==============================================================================\n",
        "print(\"\\nCalculating default timing (Assumption: Default = Last Payment + 1 month)...\")\n",
        "\n",
        "# Create default date based on our assumption\n",
        "defaulted_mask = core_individual_cohort_pd['loan_status'].isin(default_statuses)\n",
        "\n",
        "# For defaulted loans: default_date = last_payment_date + 1 month\n",
        "core_individual_cohort_pd.loc[defaulted_mask, 'default_date'] = (\n",
        "    core_individual_cohort_pd.loc[defaulted_mask, 'last_pymnt_d'] + pd.DateOffset(months=1)\n",
        ")\n",
        "\n",
        "# Calculate months to default for defaulted loans\n",
        "core_individual_cohort_pd.loc[defaulted_mask, 'months_to_default'] = (\n",
        "    (core_individual_cohort_pd.loc[defaulted_mask, 'default_date'] -\n",
        "     core_individual_cohort_pd.loc[defaulted_mask, 'issue_d']).dt.days / 30.44\n",
        ").round()\n",
        "\n",
        "# For non-defaulted loans, calculate observation period\n",
        "core_individual_cohort_pd.loc[~defaulted_mask, 'observation_months'] = (\n",
        "    (snapshot_date - core_individual_cohort_pd.loc[~defaulted_mask, 'issue_d']).dt.days / 30.44\n",
        ").round()"
      ],
      "metadata": {
        "id": "eaeM75i0fzAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FILTER FOR 36-MONTH OBSERVATION WINDOW\n",
        "# ==============================================================================\n",
        "print(\"\\nFiltering for 36-month observation window...\")\n",
        "\n",
        "# Only include loans with at least 36 months of observation\n",
        "# This means loans issued before 2016-01-01 (to have 36 months by end of 2018)\n",
        "cutoff_date = pd.to_datetime('2016-01-01')\n",
        "analysis_sample_df = core_individual_cohort_pd[\n",
        "    core_individual_cohort_pd['issue_d'] < cutoff_date\n",
        "].copy()\n",
        "\n",
        "print(f\" Filtered to loans with 36+ month observation period\")\n",
        "print(f\"   Samples after filtering: {len(analysis_sample_df):,}\")\n",
        "print(f\"   Excluded recent loans: {len(core_individual_cohort_pd) - len(analysis_sample_df):,}\")"
      ],
      "metadata": {
        "id": "u7CwtfgRf0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DEFINE VINTAGES (CORRECTED ERA SPLIT)\n",
        "# ==============================================================================\n",
        "print(\"\\nDefining vintages based on temporal analysis...\")\n",
        "\n",
        "# CORRECT ERA DEFINITION: 2007-2011 vs 2012+\n",
        "analysis_sample_df['era'] = np.where(\n",
        "    analysis_sample_df['issue_d'].dt.year <= 2011,\n",
        "    'Crisis-Era (2007-2011)',\n",
        "    'Expansion Era (2012-2015)'  # Limited to 2015 for 36-month observation\n",
        ")\n",
        "\n",
        "# Calculate 36-month default flag\n",
        "analysis_sample_df['defaulted_within_36m'] = (\n",
        "    (analysis_sample_df['loan_status'].isin(default_statuses)) &\n",
        "    (analysis_sample_df['months_to_default'] <= 36)\n",
        ").astype(int)\n",
        "\n",
        "# Show vintage distribution\n",
        "vintage_dist = analysis_sample_df['era'].value_counts()\n",
        "print(f\"\\n Vintages defined:\")\n",
        "for era, count in vintage_dist.items():\n",
        "    print(f\"   {era}: {count:,} loans\")\n",
        "print()"
      ],
      "metadata": {
        "id": "T8vN2PLef2Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 3: COMPARATIVE ANALYSIS & STATISTICAL TESTING\n",
        "# ==============================================================================\n",
        "print(\"STEP 3: Comparative Analysis & Statistical Testing\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Initialize results storage\n",
        "vintage_results = []\n",
        "eras = ['Crisis-Era (2007-2011)', 'Expansion Era (2012-2015)']\n",
        "\n",
        "# Analyze each vintage\n",
        "for era in eras:\n",
        "    era_df = analysis_sample_df[analysis_sample_df['era'] == era]\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'Era': era,\n",
        "        'Sample Size': len(era_df),\n",
        "\n",
        "        # FICO Score Statistics\n",
        "        'Avg FICO Score': era_df['fico_range_high'].mean(),\n",
        "        'FICO Std Dev': era_df['fico_range_high'].std(),\n",
        "        'FICO 25th Pct': era_df['fico_range_high'].quantile(0.25),\n",
        "        'FICO Median': era_df['fico_range_high'].quantile(0.50),\n",
        "        'FICO 75th Pct': era_df['fico_range_high'].quantile(0.75),\n",
        "\n",
        "        # DTI Statistics\n",
        "        'Avg DTI': era_df['dti'].mean(),\n",
        "        'DTI Std Dev': era_df['dti'].std(),\n",
        "        'DTI Median': era_df['dti'].quantile(0.50),\n",
        "\n",
        "        # Pricing & Performance\n",
        "        'Avg Interest Rate': era_df['int_rate'].mean(),\n",
        "        '36m Default Rate (%)': (era_df['defaulted_within_36m'].sum() / len(era_df)) * 100,\n",
        "        'Total Defaults': era_df['defaulted_within_36m'].sum()\n",
        "    }\n",
        "    vintage_results.append(results)\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(vintage_results).set_index('Era').T"
      ],
      "metadata": {
        "id": "zVDqnzJTf4UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STATISTICAL SIGNIFICANCE TESTING\n",
        "# ==============================================================================\n",
        "print(\"\\nPerforming statistical significance tests...\")\n",
        "\n",
        "crisis_df = analysis_sample_df[analysis_sample_df['era'] == eras[0]]\n",
        "expansion_df = analysis_sample_df[analysis_sample_df['era'] == eras[1]]\n",
        "\n",
        "# T-tests for continuous variables\n",
        "_, p_val_fico = ttest_ind(\n",
        "    crisis_df['fico_range_high'].dropna(),\n",
        "    expansion_df['fico_range_high'].dropna()\n",
        ")\n",
        "_, p_val_dti = ttest_ind(\n",
        "    crisis_df['dti'].dropna(),\n",
        "    expansion_df['dti'].dropna()\n",
        ")\n",
        "_, p_val_int_rate = ttest_ind(\n",
        "    crisis_df['int_rate'].dropna(),\n",
        "    expansion_df['int_rate'].dropna()\n",
        ")\n",
        "\n",
        "# Chi-square test for default rates\n",
        "contingency_table = pd.crosstab(\n",
        "    analysis_sample_df['era'],\n",
        "    analysis_sample_df['defaulted_within_36m']\n",
        ")\n",
        "_, p_val_default, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "# Create p-value summary\n",
        "p_values = {\n",
        "    'Avg FICO Score': p_val_fico,\n",
        "    'Avg DTI': p_val_dti,\n",
        "    'Avg Interest Rate': p_val_int_rate,\n",
        "    '36m Default Rate (%)': p_val_default\n",
        "}"
      ],
      "metadata": {
        "id": "xPCW7mFvf6aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DISPLAY RESULTS TABLE\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FORENSIC SUMMARY TABLE: Vintage Comparison\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a clean display table\n",
        "display_df = summary_df.copy()\n",
        "\n",
        "# Add significance indicators\n",
        "for metric, p_val in p_values.items():\n",
        "    if metric in display_df.index:\n",
        "        sig_marker = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
        "        if sig_marker:\n",
        "            display_df.loc[metric, 'Expansion Era (2012-2015)'] = f\"{display_df.loc[metric, 'Expansion Era (2012-2015)']:.2f}{sig_marker}\"\n",
        "\n",
        "print(display_df)\n",
        "print(\"\\n* p<0.05, ** p<0.01, *** p<0.001\")\n",
        "print()\n",
        "\n",
        "# Print key findings\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Calculate differences\n",
        "fico_diff = summary_df.loc['Avg FICO Score', 'Expansion Era (2012-2015)'] - summary_df.loc['Avg FICO Score', 'Crisis-Era (2007-2011)']\n",
        "dti_diff = summary_df.loc['Avg DTI', 'Expansion Era (2012-2015)'] - summary_df.loc['Avg DTI', 'Crisis-Era (2007-2011)']\n",
        "rate_diff = summary_df.loc['Avg Interest Rate', 'Expansion Era (2012-2015)'] - summary_df.loc['Avg Interest Rate', 'Crisis-Era (2007-2011)']\n",
        "default_diff = summary_df.loc['36m Default Rate (%)', 'Expansion Era (2012-2015)'] - summary_df.loc['36m Default Rate (%)', 'Crisis-Era (2007-2011)']\n",
        "\n",
        "print(f\" FICO Score Change: {fico_diff:+.1f} points\")\n",
        "print(f\" DTI Change: {dti_diff:+.1f} percentage points\")\n",
        "print(f\" Interest Rate Change: {rate_diff:+.1f} percentage points\")\n",
        "print(f\" Default Rate Change: {default_diff:+.2f} percentage points\")\n",
        "print()"
      ],
      "metadata": {
        "id": "QRt1adEef9Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 4: VISUALIZATION\n",
        "# ==============================================================================\n",
        "print(\"\\n STEP 4: Generating Forensic Visualizations\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Prepare data for plotting\n",
        "plot_metrics = ['Avg FICO Score', 'Avg DTI', 'Avg Interest Rate', '36m Default Rate (%)']\n",
        "plot_data = summary_df.loc[plot_metrics].T.reset_index()\n",
        "plot_data.rename(columns={'index': 'Era'}, inplace=True)\n",
        "\n",
        "# US Unemployment Rate Data (for macroeconomic context)\n",
        "unemployment_data = {\n",
        "    2007: 4.6, 2008: 5.8, 2009: 9.3, 2010: 9.6, 2011: 8.9,\n",
        "    2012: 8.1, 2013: 7.4, 2014: 6.2, 2015: 5.3, 2016: 4.9, 2017: 4.4\n",
        "}\n",
        "unemployment_df = pd.DataFrame(\n",
        "    list(unemployment_data.items()),\n",
        "    columns=['Year', 'Unemployment Rate (%)']\n",
        ")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    row_heights=[0.7, 0.3],\n",
        "    vertical_spacing=0.12,\n",
        "    subplot_titles=(\n",
        "        \"<b>Vintage Performance: Crisis-Era vs. Expansion Era</b>\",\n",
        "        \"<b>Macroeconomic Context: U.S. Unemployment Rate</b>\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Color scheme\n",
        "colors = {\n",
        "    'Avg FICO Score': '#1f77b4',\n",
        "    'Avg DTI': '#ff7f0e',\n",
        "    'Avg Interest Rate': '#2ca02c',\n",
        "    '36m Default Rate (%)': '#d62728'\n",
        "}\n",
        "\n",
        "# Panel 1: Comparative Bar Chart\n",
        "x_positions = np.arange(len(plot_metrics))\n",
        "width = 0.35\n",
        "\n",
        "for i, era in enumerate(plot_data['Era']):\n",
        "    values = plot_data.loc[plot_data['Era'] == era, plot_metrics].values[0]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            name=era,\n",
        "            x=plot_metrics,\n",
        "            y=values,\n",
        "            text=[f'{v:.2f}' if v < 100 else f'{v:.0f}' for v in values],\n",
        "            textposition='outside',\n",
        "            marker_color='#1f77b4' if 'Crisis' in era else '#ff7f0e',\n",
        "            offsetgroup=i\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Panel 2: Unemployment Rate Timeline\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=unemployment_df['Year'],\n",
        "        y=unemployment_df['Unemployment Rate (%)'],\n",
        "        mode='lines+markers',\n",
        "        name='Unemployment Rate',\n",
        "        line=dict(color='#7f7f7f', width=2),\n",
        "        marker=dict(size=8),\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Add vertical line at 2012 (Great Data Enrichment)\n",
        "fig.add_vline(\n",
        "    x=2012,\n",
        "    line_width=2,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"red\",\n",
        "    annotation_text=\"2012 Data Enrichment\",\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Shade the two eras\n",
        "fig.add_vrect(\n",
        "    x0=2007, x1=2011.5,\n",
        "    fillcolor=\"blue\", opacity=0.1,\n",
        "    line_width=0,\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_vrect(\n",
        "    x0=2011.5, x1=2017.5,\n",
        "    fillcolor=\"orange\", opacity=0.1,\n",
        "    line_width=0,\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title_text=\"<b>EXHIBIT A-2: FORENSIC ANALYSIS OF UNDERWRITING ERAS</b><br><sup>Impact Assessment of the 2012 'Great Data Enrichment' Initiative</sup>\",\n",
        "    title_font_size=16,\n",
        "    barmode='group',\n",
        "    showlegend=True,\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    ),\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "# Update axes\n",
        "fig.update_xaxes(title_text=\"Metrics\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Year\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Unemployment Rate (%)\", row=2, col=1)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wLEZattWfQ8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 5: ADDITIONAL ANALYSIS - DISTRIBUTION COMPARISON\n",
        "# ==============================================================================\n",
        "print(\"\\n STEP 5: Distribution Analysis\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create distribution comparison plots\n",
        "fig_dist = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        'FICO Score Distribution',\n",
        "        'DTI Distribution',\n",
        "        'Interest Rate Distribution',\n",
        "        'Default Rate by Loan Grade'\n",
        "    )\n",
        ")\n",
        "\n",
        "# FICO Distribution\n",
        "for era in eras:\n",
        "    era_data = analysis_sample_df[analysis_sample_df['era'] == era]['fico_range_high'].dropna()\n",
        "    fig_dist.add_trace(\n",
        "        go.Histogram(\n",
        "            x=era_data,\n",
        "            name=era.split(' ')[0],  # Shortened name\n",
        "            opacity=0.6,\n",
        "            nbinsx=30\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# DTI Distribution\n",
        "for era in eras:\n",
        "    era_data = analysis_sample_df[analysis_sample_df['era'] == era]['dti'].dropna()\n",
        "    fig_dist.add_trace(\n",
        "        go.Histogram(\n",
        "            x=era_data,\n",
        "            name=era.split(' ')[0],\n",
        "            opacity=0.6,\n",
        "            nbinsx=30\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Interest Rate Distribution\n",
        "for era in eras:\n",
        "    era_data = analysis_sample_df[analysis_sample_df['era'] == era]['int_rate'].dropna()\n",
        "    fig_dist.add_trace(\n",
        "        go.Histogram(\n",
        "            x=era_data,\n",
        "            name=era.split(' ')[0],\n",
        "            opacity=0.6,\n",
        "            nbinsx=30\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# Default Rate by Grade\n",
        "grade_defaults = analysis_sample_df.groupby(['era', 'grade'])['defaulted_within_36m'].agg(['mean', 'count'])\n",
        "grade_defaults['mean'] = grade_defaults['mean'] * 100  # Convert to percentage\n",
        "\n",
        "for era in eras:\n",
        "    era_grades = grade_defaults.loc[era]\n",
        "    era_grades = era_grades[era_grades['count'] > 100]  # Filter for sufficient sample size\n",
        "    fig_dist.add_trace(\n",
        "        go.Bar(\n",
        "            x=era_grades.index,\n",
        "            y=era_grades['mean'],\n",
        "            name=era.split(' ')[0],\n",
        "            text=[f'{v:.1f}%' for v in era_grades['mean']],\n",
        "            textposition='outside'\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "# Update layout\n",
        "fig_dist.update_layout(\n",
        "    height=700,\n",
        "    title_text=\"<b>Distribution Comparison: Crisis-Era vs. Expansion Era</b>\",\n",
        "    showlegend=True,\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "# Update axes labels\n",
        "fig_dist.update_xaxes(title_text=\"FICO Score\", row=1, col=1)\n",
        "fig_dist.update_xaxes(title_text=\"DTI (%)\", row=1, col=2)\n",
        "fig_dist.update_xaxes(title_text=\"Interest Rate (%)\", row=2, col=1)\n",
        "fig_dist.update_xaxes(title_text=\"Loan Grade\", row=2, col=2)\n",
        "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
        "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
        "fig_dist.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
        "fig_dist.update_yaxes(title_text=\"Default Rate (%)\", row=2, col=2)\n",
        "\n",
        "fig_dist.show()"
      ],
      "metadata": {
        "id": "uAvLcCIAgE4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# STEP 6: ROBUSTNESS CHECK - ALTERNATIVE OBSERVATION WINDOWS\n",
        "# ==============================================================================\n",
        "print(\"\\n STEP 6: Robustness Check - Alternative Observation Windows\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Test with 24-month and 48-month windows\n",
        "robustness_results = []\n",
        "\n",
        "for window in [24, 36, 48]:\n",
        "    # Calculate defaults within window\n",
        "    temp_df = analysis_sample_df.copy()\n",
        "    temp_df[f'defaulted_within_{window}m'] = (\n",
        "        (temp_df['loan_status'].isin(default_statuses)) &\n",
        "        (temp_df['months_to_default'] <= window)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Calculate default rates by era\n",
        "    for era in eras:\n",
        "        era_df = temp_df[temp_df['era'] == era]\n",
        "        default_rate = (era_df[f'defaulted_within_{window}m'].sum() / len(era_df)) * 100\n",
        "\n",
        "        robustness_results.append({\n",
        "            'Window': f'{window} months',\n",
        "            'Era': era,\n",
        "            'Default Rate (%)': default_rate,\n",
        "            'Sample Size': len(era_df)\n",
        "        })\n",
        "\n",
        "robustness_df = pd.DataFrame(robustness_results).pivot(\n",
        "    index='Window',\n",
        "    columns='Era',\n",
        "    values='Default Rate (%)'\n",
        ")\n",
        "\n",
        "print(\"\\nDefault Rates Across Different Observation Windows:\")\n",
        "print(robustness_df)\n",
        "print()"
      ],
      "metadata": {
        "id": "aZ49KBg6fQ5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}